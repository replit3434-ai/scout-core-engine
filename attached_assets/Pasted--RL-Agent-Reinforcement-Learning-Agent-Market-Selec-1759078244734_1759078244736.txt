"""
RL Agent (Reinforcement Learning Agent)
======================================

Market Selector'dan gelen aday sinyalleri ge√ßmi≈ü performans ve 
√∂d√ºl/ceza mantƒ±ƒüƒ± ile deƒüerlendirir, final sinyali √ºretir.
"""

import logging
import numpy as np
import json
import pickle
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, deque


class RLAgent:
    """
    Reinforcement Learning tabanlƒ± sinyal karar motoru
    """
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger("rl_agent")
        
        # RL parametreleri
        self.learning_rate = 0.01
        self.discount_factor = 0.95
        self.epsilon = 0.1  # Exploration rate
        self.epsilon_decay = 0.995
        self.min_epsilon = 0.01
        
        # State ve action spaces
        self.state_features = [
            'minute', 'score_diff', 'shots_ratio', 'possession_diff',
            'corners_ratio', 'fouls_ratio', 'momentum_home', 'momentum_away',
            'market_confidence', 'data_quality', 'league_strength'
        ]
        
        self.actions = [
            'generate_signal', 'skip_signal', 'wait_more_data'
        ]
        
        # Q-table ve experience replay
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.experience_buffer = deque(maxlen=10000)
        self.signal_history = deque(maxlen=1000)
        
        # Performance tracking
        self.performance_metrics = {
            'total_signals': 0,
            'successful_signals': 0,
            'failed_signals': 0,
            'win_rate': 0.0,
            'avg_confidence': 0.0,
            'market_performance': defaultdict(lambda: {'wins': 0, 'total': 0})
        }
        
        # Model dosyasƒ±
        self.model_file = "rl_agent_model.pkl"
        self._load_model()
        
        self.logger.info("ü§ñ RL Agent ba≈ülatƒ±ldƒ±")
    
    async def generate_signals(self, combined_data: Dict, candidate_markets: List[Dict]) -> List[Dict]:
        """
        Aday marketleri deƒüerlendirerek final sinyalleri √ºret
        
        Args:
            combined_data: Birle≈üik ma√ß verisi
            candidate_markets: Market Selector'dan gelen aday marketler
            
        Returns:
            Final sinyal listesi
        """
        try:
            if not candidate_markets:
                return []
            
            final_signals = []
            
            for candidate in candidate_markets:
                # State'i hesapla
                state = self._extract_state(combined_data, candidate)
                
                # Action se√ß (RL decision)
                action = self._select_action(state, candidate)
                
                if action == 'generate_signal':
                    # Sinyal √ºret
                    signal = self._create_signal(combined_data, candidate, state)
                    final_signals.append(signal)
                    
                    # Experience'ƒ± kaydet
                    self._record_experience(state, action, candidate)
                    
                elif action == 'wait_more_data':
                    # Daha fazla veri bekle - d√º≈ü√ºk g√ºvenle sinyal √ºret
                    candidate['confidence'] *= 0.8
                    if candidate['confidence'] > 0.4:
                        signal = self._create_signal(combined_data, candidate, state)
                        signal['status'] = 'waiting'
                        final_signals.append(signal)
                
                # Skip durumunda hi√ßbir ≈üey yapma
            
            # Model g√ºncelle
            if len(self.experience_buffer) > 100:
                self._update_model()
            
            self.logger.debug(f"ü§ñ RL Agent: {len(final_signals)} final sinyal √ºretildi")
            return final_signals
            
        except Exception as e:
            self.logger.error(f"‚ùå RL Agent sinyal √ºretimi hatasƒ±: {e}")
            return []
    
    def _extract_state(self, combined_data: Dict, candidate: Dict) -> np.ndarray:
        """
        Mevcut durumdan state vector'√º √ßƒ±kar
        
        Args:
            combined_data: Ma√ß verisi
            candidate: Aday market
            
        Returns:
            State vector
        """
        try:
            match_data = combined_data['match']
            
            # Temel ma√ß bilgileri
            minute = match_data.get('time', {}).get('minute', 0) / 90.0  # Normalize
            
            # Skor farkƒ±
            scores = match_data.get('scores', [])
            home_score = away_score = 0
            for score in scores:
                if score.get('description') == 'CURRENT':
                    home_score = score.get('score', {}).get('localteam', 0)
                    away_score = score.get('score', {}).get('visitorteam', 0)
            
            score_diff = (home_score - away_score) / 5.0  # Normalize
            
            # ƒ∞statistikler
            stats = match_data.get('statistics', {})
            home_stats = stats.get('home', {})
            away_stats = stats.get('away', {})
            
            # ≈ûut oranƒ±
            home_shots = home_stats.get('shots_total', 0)
            away_shots = away_stats.get('shots_total', 0)
            total_shots = home_shots + away_shots
            shots_ratio = (home_shots / total_shots) if total_shots > 0 else 0.5
            
            # Possession farkƒ±
            home_poss = home_stats.get('possession', 50)
            away_poss = away_stats.get('possession', 50)
            possession_diff = (home_poss - away_poss) / 100.0
            
            # Korner oranƒ±
            home_corners = home_stats.get('corners', 0)
            away_corners = away_stats.get('corners', 0)
            total_corners = home_corners + away_corners
            corners_ratio = (home_corners / total_corners) if total_corners > 0 else 0.5
            
            # Faul oranƒ±
            home_fouls = home_stats.get('fouls', 0)
            away_fouls = away_stats.get('fouls', 0)
            total_fouls = home_fouls + away_fouls
            fouls_ratio = (home_fouls / total_fouls) if total_fouls > 0 else 0.5
            
            # Momentum (basit hesaplama)
            momentum_home = self._calculate_simple_momentum(match_data, 'home')
            momentum_away = self._calculate_simple_momentum(match_data, 'away')
            
            # Market √∂zellikleri
            market_confidence = candidate.get('confidence', 0.5)
            data_quality = candidate.get('data_quality', 0.5)
            
            # Lig g√ºc√º (basit)
            league_strength = 0.7  # Placeholder
            
            state = np.array([
                minute, score_diff, shots_ratio, possession_diff,
                corners_ratio, fouls_ratio, momentum_home, momentum_away,
                market_confidence, data_quality, league_strength
            ])
            
            return state
            
        except Exception as e:
            self.logger.error(f"‚ùå State extraction hatasƒ±: {e}")
            return np.zeros(len(self.state_features))
    
    def _select_action(self, state: np.ndarray, candidate: Dict) -> str:
        """
        RL policy ile action se√ß
        
        Args:
            state: Mevcut state
            candidate: Aday market
            
        Returns:
            Se√ßilen action
        """
        try:
            # State'i string'e √ßevir (Q-table i√ßin)
            state_key = self._state_to_key(state)
            
            # Epsilon-greedy policy
            if np.random.random() < self.epsilon:
                # Exploration
                action = np.random.choice(self.actions)
            else:
                # Exploitation - en iyi Q-value'yu se√ß
                q_values = [self.q_table[state_key][action] for action in self.actions]
                best_action_idx = np.argmax(q_values)
                action = self.actions[best_action_idx]
            
            # G√ºven seviyesi kontrol√º
            confidence = candidate.get('confidence', 0.5)
            
            if confidence < 0.4:
                action = 'skip_signal'
            elif confidence < 0.6 and action == 'generate_signal':
                action = 'wait_more_data'
            
            return action
            
        except Exception as e:
            self.logger.error(f"‚ùå Action selection hatasƒ±: {e}")
            return 'skip_signal'
    
    def _create_signal(self, combined_data: Dict, candidate: Dict, state: np.ndarray) -> Dict:
        """
        Final sinyal olu≈ütur
        
        Args:
            combined_data: Ma√ß verisi
            candidate: Aday market
            state: State vector
            
        Returns:
            Final sinyal
        """
        try:
            match_data = combined_data['match']
            
            # RL confidence adjustment
            rl_confidence = self._calculate_rl_confidence(state, candidate)
            final_confidence = (candidate['confidence'] + rl_confidence) / 2
            
            signal = {
                'id': f"signal_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{np.random.randint(1000, 9999)}",
                'match_id': match_data.get('id'),
                'league_id': match_data.get('league_id'),
                'market': candidate['market'],
                'selection': candidate['selection'],
                'confidence': final_confidence,
                'original_confidence': candidate['confidence'],
                'rl_confidence': rl_confidence,
                'reasoning': candidate.get('reasoning', ''),
                'rl_reasoning': self._generate_rl_reasoning(state, candidate),
                'data_quality': candidate.get('data_quality', 0.5),
                'created_at': datetime.now(),
                'status': 'active',
                'source': 'rl_agent_v6',
                'state_features': state.tolist(),
                'expected_outcome': None,  # Sonu√ß takibi i√ßin
                'actual_outcome': None
            }
            
            # Signal history'ye ekle
            self.signal_history.append(signal.copy())
            
            # Performance tracking
            self.performance_metrics['total_signals'] += 1
            
            return signal
            
        except Exception as e:
            self.logger.error(f"‚ùå Sinyal olu≈üturma hatasƒ±: {e}")
            return {}
    
    def _calculate_rl_confidence(self, state: np.ndarray, candidate: Dict) -> float:
        """
        RL modelinden confidence hesapla
        
        Args:
            state: State vector
            candidate: Aday market
            
        Returns:
            RL confidence score
        """
        try:
            state_key = self._state_to_key(state)
            market_type = candidate['market']
            
            # Market-specific performance
            market_perf = self.performance_metrics['market_performance'][market_type]
            market_win_rate = (market_perf['wins'] / market_perf['total']) if market_perf['total'] > 0 else 0.5
            
            # Q-value based confidence
            q_value = self.q_table[state_key]['generate_signal']
            q_confidence = 1 / (1 + np.exp(-q_value))  # Sigmoid
            
            # Combine factors
            rl_confidence = (market_win_rate * 0.6) + (q_confidence * 0.4)
            
            return np.clip(rl_confidence, 0.1, 0.9)
            
        except Exception as e:
            self.logger.error(f"‚ùå RL confidence hesaplama hatasƒ±: {e}")
            return 0.5
    
    def _generate_rl_reasoning(self, state: np.ndarray, candidate: Dict) -> str:
        """
        RL kararƒ± i√ßin a√ßƒ±klama √ºret
        
        Args:
            state: State vector
            candidate: Aday market
            
        Returns:
            RL reasoning string
        """
        try:
            market_type = candidate['market']
            market_perf = self.performance_metrics['market_performance'][market_type]
            
            if market_perf['total'] > 0:
                win_rate = market_perf['wins'] / market_perf['total']
                return f"RL Model: {market_type} ge√ßmi≈ü ba≈üarƒ± oranƒ± %{win_rate*100:.1f}"
            else:
                return f"RL Model: {market_type} i√ßin yeni √∂ƒürenme"
                
        except Exception:
            return "RL Model: Standart deƒüerlendirme"
    
    def _calculate_simple_momentum(self, match_data: Dict, team: str) -> float:
        """
        Basit momentum hesaplama
        
        Args:
            match_data: Ma√ß verisi
            team: 'home' veya 'away'
            
        Returns:
            Momentum score (0-1)
        """
        try:
            stats = match_data.get('statistics', {}).get(team, {})
            
            # Basit momentum fakt√∂rleri
            attacks = stats.get('attacks', 0)
            shots = stats.get('shots_total', 0)
            corners = stats.get('corners', 0)
            
            # Normalize et
            momentum = (attacks / 100) + (shots / 20) + (corners / 10)
            return np.clip(momentum / 3, 0, 1)
            
        except Exception:
            return 0.5
    
    def _state_to_key(self, state: np.ndarray) -> str:
        """
        State vector'√º Q-table key'ine √ßevir
        
        Args:
            state: State vector
            
        Returns:
            State key string
        """
        try:
            # State'i discrete bins'e b√∂l
            discretized = np.round(state * 10).astype(int)
            return '_'.join(map(str, discretized))
            
        except Exception:
            return "default_state"
    
    def _record_experience(self, state: np.ndarray, action: str, candidate: Dict):
        """
        Experience'ƒ± kaydet
        
        Args:
            state: State vector
            action: Se√ßilen action
            candidate: Aday market
        """
        try:
            experience = {
                'state': state.copy(),
                'action': action,
                'market': candidate['market'],
                'confidence': candidate['confidence'],
                'timestamp': datetime.now()
            }
            
            self.experience_buffer.append(experience)
            
        except Exception as e:
            self.logger.error(f"‚ùå Experience kayƒ±t hatasƒ±: {e}")
    
    def _update_model(self):
        """
        Q-table'ƒ± g√ºncelle (basit Q-learning)
        """
        try:
            # Son experience'larƒ± al
            recent_experiences = list(self.experience_buffer)[-50:]
            
            for exp in recent_experiences:
                state_key = self._state_to_key(exp['state'])
                action = exp['action']
                
                # Reward hesapla (placeholder - ger√ßek sonu√ßlarla g√ºncellenecek)
                reward = self._calculate_reward(exp)
                
                # Q-value g√ºncelle
                old_q = self.q_table[state_key][action]
                self.q_table[state_key][action] = old_q + self.learning_rate * (reward - old_q)
            
            # Epsilon decay
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            # Model kaydet
            self._save_model()
            
        except Exception as e:
            self.logger.error(f"‚ùå Model g√ºncelleme hatasƒ±: {e}")
    
    def _calculate_reward(self, experience: Dict) -> float:
        """
        Experience i√ßin reward hesapla
        
        Args:
            experience: Experience dictionary
            
        Returns:
            Reward value
        """
        try:
            # Basit reward sistemi
            action = experience['action']
            confidence = experience['confidence']
            
            if action == 'generate_signal':
                # Y√ºksek g√ºvenli sinyaller i√ßin pozitif reward
                return confidence - 0.5
            elif action == 'skip_signal':
                # D√º≈ü√ºk g√ºvenli sinyalleri skip etmek i√ßin pozitif reward
                return 0.5 - confidence
            else:  # wait_more_data
                return 0.1
                
        except Exception:
            return 0.0
    
    def update_signal_outcome(self, signal_id: str, outcome: bool, profit: float = 0.0):
        """
        Sinyal sonucunu g√ºncelle (ger√ßek sonu√ß geldiƒüinde)
        
        Args:
            signal_id: Sinyal ID'si
            outcome: True/False (ba≈üarƒ±lƒ±/ba≈üarƒ±sƒ±z)
            profit: Kar/zarar miktarƒ±
        """
        try:
            # Signal history'de bul ve g√ºncelle
            for signal in self.signal_history:
                if signal.get('id') == signal_id:
                    signal['actual_outcome'] = outcome
                    signal['profit'] = profit
                    
                    # Performance metrics g√ºncelle
                    if outcome:
                        self.performance_metrics['successful_signals'] += 1
                        market_type = signal['market']
                        self.performance_metrics['market_performance'][market_type]['wins'] += 1
                    else:
                        self.performance_metrics['failed_signals'] += 1
                    
                    # Win rate g√ºncelle
                    total = self.performance_metrics['successful_signals'] + self.performance_metrics['failed_signals']
                    if total > 0:
                        self.performance_metrics['win_rate'] = self.performance_metrics['successful_signals'] / total
                    
                    # Market performance g√ºncelle
                    market_type = signal['market']
                    self.performance_metrics['market_performance'][market_type]['total'] += 1
                    
                    self.logger.info(f"üìä Sinyal {signal_id} sonucu g√ºncellendi: {outcome}")
                    break
            
            # Model'i yeniden eƒüit
            self._retrain_with_outcome(signal_id, outcome, profit)
            
        except Exception as e:
            self.logger.error(f"‚ùå Sinyal sonucu g√ºncelleme hatasƒ±: {e}")
    
    def _retrain_with_outcome(self, signal_id: str, outcome: bool, profit: float):
        """
        Ger√ßek sonu√ßla model'i yeniden eƒüit
        
        Args:
            signal_id: Sinyal ID'si
            outcome: Sonu√ß
            profit: Kar/zarar
        """
        try:
            # ƒ∞lgili experience'ƒ± bul
            for exp in self.experience_buffer:
                # Experience ile signal'i e≈üle≈ütir (timestamp bazlƒ±)
                # Bu kƒ±sƒ±m daha detaylƒ± implementasyon gerektirir
                pass
            
            # Q-table'ƒ± ger√ßek sonu√ßla g√ºncelle
            # Bu kƒ±sƒ±m da daha detaylƒ± implementasyon gerektirir
            
        except Exception as e:
            self.logger.error(f"‚ùå Outcome-based retraining hatasƒ±: {e}")
    
    def get_performance_report(self) -> Dict:
        """
        Performance raporu al
        
        Returns:
            Performance metrics
        """
        try:
            report = self.performance_metrics.copy()
            
            # Market-specific win rates
            market_details = {}
            for market, perf in self.performance_metrics['market_performance'].items():
                if perf['total'] > 0:
                    market_details[market] = {
                        'win_rate': perf['wins'] / perf['total'],
                        'total_signals': perf['total'],
                        'wins': perf['wins']
                    }
            
            report['market_details'] = market_details
            report['model_parameters'] = {
                'epsilon': self.epsilon,
                'learning_rate': self.learning_rate,
                'q_table_size': len(self.q_table)
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå Performance raporu hatasƒ±: {e}")
            return {}
    
    def _save_model(self):
        """Model'i dosyaya kaydet"""
        try:
            model_data = {
                'q_table': dict(self.q_table),
                'performance_metrics': self.performance_metrics,
                'epsilon': self.epsilon,
                'signal_history': list(self.signal_history)
            }
            
            with open(self.model_file, 'wb') as f:
                pickle.dump(model_data, f)
                
        except Exception as e:
            self.logger.error(f"‚ùå Model kaydetme hatasƒ±: {e}")
    
    def _load_model(self):
        """Model'i dosyadan y√ºkle"""
        try:
            with open(self.model_file, 'rb') as f:
                model_data = pickle.load(f)
                
                self.q_table = defaultdict(lambda: defaultdict(float), model_data.get('q_table', {}))
                self.performance_metrics = model_data.get('performance_metrics', self.performance_metrics)
                self.epsilon = model_data.get('epsilon', self.epsilon)
                
                # Signal history'yi deque'ya √ßevir
                history_data = model_data.get('signal_history', [])
                self.signal_history = deque(history_data, maxlen=1000)
                
                self.logger.info(f"ü§ñ RL Model y√ºklendi - Q-table size: {len(self.q_table)}")
                
        except FileNotFoundError:
            self.logger.info("ü§ñ Yeni RL Model ba≈ülatƒ±lƒ±yor")
        except Exception as e:
            self.logger.error(f"‚ùå Model y√ºkleme hatasƒ±: {e}")
    
    def reset_model(self):
        """Model'i sƒ±fƒ±rla"""
        try:
            self.q_table = defaultdict(lambda: defaultdict(float))
            self.experience_buffer.clear()
            self.signal_history.clear()
            self.performance_metrics = {
                'total_signals': 0,
                'successful_signals': 0,
                'failed_signals': 0,
                'win_rate': 0.0,
                'avg_confidence': 0.0,
                'market_performance': defaultdict(lambda: {'wins': 0, 'total': 0})
            }
            self.epsilon = 0.1
            
            self.logger.info("ü§ñ RL Model sƒ±fƒ±rlandƒ±")
            
        except Exception as e:
            self.logger.error(f"‚ùå Model sƒ±fƒ±rlama hatasƒ±: {e}")